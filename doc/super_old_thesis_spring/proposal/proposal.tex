\documentclass[12pt]{article}
\usepackage{amsfonts}
\title{Thesis Proposal:  Classes of Artificial Neural Networks at the \textit{Edge of Chaos}}
\author{Ralph Bean\\
Department of Computer Science\\
Rochester Institute of Technology\\
\texttt{ralph.bean@gmail.com}}
\pagestyle{headings}
\begin{document}
\maketitle
\vfill
\textbf{Thesis Committe}

\textbf{Advisor:}\hspace{14pt}Dr. Roger Gaborski

\textbf{Observer:}\hspace{7pt}Dr. Peter Anderson

\textbf{Reader:}\hspace{18pt}Thomas Borelli


\newpage
\setcounter{secnumdepth}{3}

\tableofcontents
\newpage

\section{Introduction}
The computational tasks of nonlinear classification and control are of high
importance to those interested in vision and acoustics.  The information
of interest in real world signals if most often embedded intrinsically in
their temporal structure.  Examples include distinguishing between and
producing the subtleties of human speech (cite), environment mapping (cite),
and predicting natural time series such as weather patterns (cite) and 
radioactive decay (cite).

For many tasks, humans outcompete state-of-the-art computational techniques
hands-down.

Consequently, techniques regarding recurrent neural networks 
are of interest to computing research as a potential solution which could
harness the hard work already done for us by evolution.  

Note about difficulties
Note about Jaeger
Point out confusion about edge of chaos
Point out the point of this thesis (to clarify and explain edge of chaos).


\section{Artificial Neural Networks (ANNs)}
Artificial Neural Networks are a computational model of natural brains that
consist of a network of "neurons".  These artificial neuron models activate 
more or less in response to
each other and a series of inputs as governed by a set of equations.  
They are perhaps best contrasted with
the Biological Neural Network or Spike/Pulsed Neural Network computing models
which are more biologically realistic but more computationally expensive and
more theoretically unwieldy.

TODO -- continue

\section{Chaos}

Dynamical systems theory is a branch of mathematics that deals with systems 
of change rules and the ultimate fate of solutions to those systems.  A system
which has orbits which do not leave a particular interval and are not periodic
are called chaotic -- and chaotic systems have many interesting properties.

\section{Chaos in ANNs}
\subsection{Autonomous ANNs}
My stated purpose at the beginning of the summer was to "derive constraints
on the
parameters of a neural network such that the dynamics of the network under
no input (i.e. the baseline behavior) are chaotic."  In July,
I informally proved that such there are no such
solutions in the parameter space of an artificial (discrete-time) neural
network which makes use of the sigmoid (hyperbolic tangent) activation function
alone.  To be discussed more later, the choice of an activation function
dominates the dynamics of no-input networks over their histories.

\subsubsection{Single-neuron Networks}

Chaos in one-dimensional discrete-time dynamical systems is much more well
understood than in any other non-trivial class of systems.  Proof that
a new one-dimensional system is chaotic may be done by the Sarkovskii's Theorem
which, in its condensed form, states that any one-dimensional discrete-time dynamical system which
has a periodic orbit of prime period $3$, has periodic orbits of every other prime period.  The natural numbers are countably infinite.  An infinite number of
distinct periodic orbits on an open interval implies density (much like the argument for density of the rationals) and density of periodic orbits is the very topological definition of chaos.

Our one-neuron recurrent network can be expressed as a dynamical system:
$$x_{t+1} = f(x_{t}) = tanh(w*x_{t} + b)$$
where $x_{t}$ is the state of the neuron at time $t$, $w$ is the weight of the connection from this neuron to itself, and $b$ is the bias of the neuron towards activation.  We have a one-dimensional phase space and a two-dimensional parameter space.

One method I had my hopes on at the beginning of the summer was to use Sarkovskii's Thereom on the above equation and solve for $x = f(f(f(x)))$ and say "See, the equality only holds if $w$ and $b$ take on 'this' relationship."..
However, due to our neuron's activation function being transcendtal and
consequently, non-algebraic, $x = f(f(f(x)))$ eluded manipulation.
(even intractable for mathematica).

An entirely different (and more often used) method of proving the existence
of chaos in a dynamical system is to show that it is topologically conjugate
to another known chaotic dynamical system such as the shift map, the smale horsehoe map,
or the logistic map.
These three are all defined quite differently but have nonetheless been.
shown to be conjugate to one another.  To show topological conjugacy.
between two dynamical systems $f(x)$ and $g(x)$, one must provide a.
function, a homeomorphism, $h(x)$ such that $h^{-1} \circ f = g \circ h$.
I hoped to provide a homeomorphism between our single-neuron artificial.
neural network and the logistic map, $x_{t+1} = \mu x (1-x)$.  
My many attempts at this failed.  Is there such an $h(x)$?

\textit{Claim:}  A network of one recurrently connected neuron in the.
absence on online input does not have a chaotic baseline for any.
combination of parameters.

\textit{Proof:}
Let our neural network's change function be denoted by $g(x)$ and the logistic map be denoted by $f(x)$.
Suppose there exists an $h$ such that $h$ is a homeomorphism.
between $f$ and $g$.  Then $h$ must map every dynamicaly unique.
orbit of $f$ onto the orbits of $g$.  Observe that the logistic map has a.
critical point ($f^{\prime} = 0$) at $x=\frac{1}{2}$ for all $\mu$.  This.
critical point divides the domain of $f$ into two distinct subintervals.
which map to a common range.  More.
specifically, $f(\frac{1}{2} + \delta_{1}) = f(\frac{1}{2} - \delta_{2})$.
where $\delta_{1} \neq \delta_{2}$ and $\delta_{1} \le \delta_{2}$ without loss of generality for infinitely many (although distinct).
pairs of $\delta_{1}$ and $\delta_{2}$.

Now note that $g^{\prime}(x) = (1 - tanh^{2}(wx+b))w \neq 0$ for all $x$ except the trivial case where $w=0$...
Consequently, for every $x \neq -\frac{b}{w}$, there can be no $y$.
such that $g(x) = g(y)$.

$$f(\frac{1}{2} + \delta_{1}) = f(\frac{1}{2} - \delta_{2})$$
$$h(f(\frac{1}{2} + \delta_{1})) = h(f(\frac{1}{2} - \delta_{2}))$$
$$h(f(\frac{1}{2} + \delta_{1})) = g(h^{-1}(\frac{1}{2} + \delta_{1})$$
$$g(h^{-1}(\frac{1}{2} + \delta_{1})) = g(h^{-1}(\frac{1}{2} + \delta_{2})$$
$$\delta_{1} = \delta_{2}$$
   Contradiction.  The only assumption was existence of $h$ therefore there 
   is no such $h$. So, $f$ and $g$ are not homeomorphic and therefore $g$ 
   is not chaotic, regardless of parameter choice.

\subsubsection{Multi-neuron Networks}

We are left crestfallen!  Can the same be the case for networks of multiple
neurons?  Initially we might guess not.  Henri Poincar\'e proved continuous
(not our discrete) dynamical systems may not be chaotic in either one or two
dimensions but may be in three or above.  We will proceed with proof by 
contradiction assuming the $h$ as above but we need first to consider the
topological possibilities of this more general case.

First of all, forgetting topology, is a homeomorphism $h$ between $f$ the one 
dimensional logistic map, and our multi-neuron network $g$ 
\textit{algebraically} possible?  Well, 
$f : \mathbb{R}^{1} \rightarrow \mathbb{R}^{1}$, 
$g : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$, where $n$ is the number of
neurons in our network and $n \neq 1$.  There is no pair $<l, m>$ such 
that $h : \mathbb{R}^{l} \rightarrow \mathbb{R}^{m}$ is valid to 
consider in $h^{-1} \circ f = g \circ h$.

Instead, let's reconsider our choice of $f$.  Other multi-dimensional discrete
systems are known to be chaotic, but the logistic map is most friendly so we
will stay close to home.  We can trivially pad  the span of the logistic
map by adding variables which are controlled by highly non-chaotic linear
combinations of the other variables.  

TODO -- show equation for this new $f_{padded(1,n)}$

We will soon show that $g$ is not congjuate with $f_{padded(1,n)}$, but even
if this is true is it sufficient to proof that $g$ has no chaotic subinterval?
No.  It is entirely conceivable that chaotic attractors exist that span
more dimensions that $1$ like those of the coupled logistic map or
the lorenz equations.

Consequently, in order to show the impossibility of chaotic solutions, we will
have to deal inductively with every possible dimension up to $n$.

TODO -- define the general coupling of logistic maps with trivial 
linear padding.

Is $g$ conjugate with $f_{padded(1,n)}$?
TODO -- show this.
Show that if $g$ is not conjugate with $f_{padded(i,n)}$, then $g$ is not conjugate with $f_{padded(i+1,n)}$.
TODO -- show this.

By induction, $g$ is not conjugate with any coupling of logistic maps equal
to or lower than its own dimension and so must not contain a chaotic attractor
of any dimension.  $g$ is not chaotic.

\subsection{Nonautonomous ANNs}
\section{Genetic Algorithm}
\subsection{Goal}
\subsection{Coevolution}
\subsection{Representation}
\subsection{Fitness Metric}
\subsubsection{Constraint Satisfaction}
\subsection{Crossover}
\subsection{Mutation}
\subsection{Implementation}
\section{Deliverables}
\section{Timeline}
\section{References}

\end{document}

